{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Toolkit(NLTK) we can say is the comprehensive python library for natural language processing and text analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization being the very basic thing which we usually do at beginning of building a application say text analyzer,summarizer etc. Tokenization is the process of splitting a string into a list of pieces or tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading a text file \n",
    "file = open(\"para_meteor.txt\",\"r\")\n",
    "para = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence tokenization\n",
    "#Importing the sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#Splitting the paragraph into sentences using sentence tokenization function\n",
    "sent_tokens = sent_tokenize(para)\n",
    "#What we actually get is a list of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module This instance has already been trained and works well for\n",
    "many European languages. So it knows what punctuation and characters mark the end of a\n",
    "sentence and the beginning of a new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance used in sent_tokenize() is actually loaded on demand from a pickle\n",
    "file. So if we are going to be tokenizing a lot and lot of the sentences, it will be more efficient to load the\n",
    "PunktSentenceTokenizer class once,and call its tokenize() method instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scientists estimate that about 48.5 tons (44 tonnes or 44,000 kilograms) of meteoritic material falls on the Earth each day.',\n",
       " 'Almost all the material is vaporized in Earth\\'s atmosphere, leaving a bright trail fondly called \"shooting stars.\"',\n",
       " 'Several meteors per hour can usually be seen on any given night.',\n",
       " 'Sometimes the number increases dramatically—these events are termed meteor showers.Meteor showes occur annually or at regular intervals as the Earth passes through the trail of dusty debris left by a comet.',\n",
       " 'Meteor showers are usually named after a star or constellation that is close to where the meteors appear in the sky.',\n",
       " 'Perhaps the most famous are the Perseids, which peak in August every year.',\n",
       " 'Every Perseid meteor is a tiny piece of the comet Swift-Tuttle, which swings by the Sun every 135 years.',\n",
       " 'Taking photographs of a meteor shower can be an exercise in patience as meteors streak across the sky quickly and unannounced,but with these tips – and some good fortune – you might be rewarded with a great photo.',\n",
       " 'These tips are meant for a DSLR or mirrorless camera, but some point-and-shoot cameras with manual controls could be used as well.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "tokenizer.tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola amigo.', 'Estoy bien.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Incase if we want to tokenize sentences from other languages what we can do is use(load) pickle file corresponding to other language\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
    "spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scientists',\n",
       " 'estimate',\n",
       " 'that',\n",
       " 'about',\n",
       " '48.5',\n",
       " 'tons',\n",
       " '(',\n",
       " '44',\n",
       " 'tonnes',\n",
       " 'or',\n",
       " '44,000',\n",
       " 'kilograms',\n",
       " ')',\n",
       " 'of',\n",
       " 'meteoritic',\n",
       " 'material',\n",
       " 'falls',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'each',\n",
       " 'day',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now tokenizing sentences into words(use word_tokenize)\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Tokenizing the sentences into words\n",
    "word_tokens = word_tokenize(sent_tokens[0])\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word_tokenize() function is a wrapper function that calls tokenize() on an\n",
    "instance of the TreebankWordTokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scientists',\n",
       " 'estimate',\n",
       " 'that',\n",
       " 'about',\n",
       " '48.5',\n",
       " 'tons',\n",
       " '(',\n",
       " '44',\n",
       " 'tonnes',\n",
       " 'or',\n",
       " '44,000',\n",
       " 'kilograms',\n",
       " ')',\n",
       " 'of',\n",
       " 'meteoritic',\n",
       " 'material',\n",
       " 'falls',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'each',\n",
       " 'day',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "#It basically separate words by spaces and punctuation,by the way it doesn't discard punctuation it keep them and let us decide whether to keep them or not\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "#It basically work by separating contractions\n",
    "w_t = tokenizer.tokenize(sent_tokens[0])\n",
    "w_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['should', \"n't\", 'and', ',', 'ca', \"n't\", '!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One of the tokenizer's most significant conventions is to separate contractions(doesn't separate punctuations)\n",
    "word_tokenize(\"shouldn't and, can't!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shouldn', \"'\", 't', 'and', ',', 'can', \"'\", 't', '!']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternative word tokenizer available is WordPunctTokenizer\n",
    "#WordPunctTokenizer\n",
    "#It splits all punctuation into separate tokens\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer_W = WordPunctTokenizer()\n",
    "tokenizer_W.tokenize(\"Shouldn't and, can't!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"shouldn't\", \"couldn't\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using regular expression for tokenization and using this we can tokenize the way we want\n",
    "#There are cases where some tokenizer are acceptable and in others, different tokenizer are acceptable\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#Now we will create an instance of reguklar expression tokenizer and give it the matching token(regular expression string)\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "#tokenizer.tokenize(\"Shouldn't couldn't\")\n",
    "#RegexpTokenizer can also work by matching the gaps, as opposed to the tokens\n",
    "\n",
    "#If we don't want to create an instance from RegexpTokenizer class\n",
    "#We can make regex work in the way we want it internally implement the re.findall() and re.split() to find the matching patterg on eich to separate the tokens\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"shouldn't couldn't\",\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"couldn't\", ',', \"shouldn't\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the simple whitespace tokenizer\n",
    "#The gaps=True parameter means that the pattern is used to identify gaps to tokenize on.If we used gaps=False,then the pattern would be used to identify tokens\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"couldn't , shouldn't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually our default sentence tokenizer works very well but sometime it is not the case where we want our text to tokenized in the way we want on some specific patterns, so there we use regex tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides a PunktSentenceTokenizer class that we can use to train on raw text to produce\n",
    "a custom sentence tokenizer. we can get raw text either by reading in a file, or from an NLTK\n",
    "corpus using the raw() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /home/bluebrain/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "sents = sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I only have a dollar...Can you spare some change?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...\\nHobo: Oh, this is all theatrical.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using an ordinary sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = sent_tokenize(text)\n",
    "sent_tokens[678]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PunktSentenceTokenizer class uses an unsupervised learning algorithm to learn\n",
    "what constitutes a sentence break. It is unsupervised because you don't have to give it any\n",
    "labeled training data, just raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization the next step is to remove stopwords,they are the words which do not contribute to the meaning of a sentence,\n",
    "at least for the purposes of information retrieval and natural language processing\n",
    "\n",
    "Most of the search engines filter out stopwords from the search queries in order to save space in their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi name sourabh , Data Science Lover\n"
     ]
    }
   ],
   "source": [
    "#The stopwords corpus is an instance of nltk.corpus.reader.WordListCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "#Creating a set of all the stopwords in english language(Word of not much importance)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#Now removing all the stop words from a text\n",
    "text = \"Hi my name is sourabh, and i am Data Science Lover\"\n",
    "list_words = word_tokenize(text)\n",
    "list_stopwords_removed = [word for word in list_words if not word in stop_words]\n",
    "text_filtered = \" \".join(list_stopwords_removed)\n",
    "print(text_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seeing the english stopwords\n",
    "stopwords.words(\"english\")\n",
    "#Similarly we can use stopwords of different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking up synsets for word in wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is a lexical database for the English language. In other words, it's a dictionary\n",
    "designed specifically for natural language processing\n",
    "\n",
    "Synset(synonymous word expressing same concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a small group of indispensable persons or things'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"core\")[0]\n",
    "#Some synset also have example method\n",
    "#Basically it return us a list of synonymous words\n",
    "#syn.name()\n",
    "#Looking for the synset of core or can look for others\n",
    "syn.definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synsets are organized in a structure similar to that of an inheritance tree. More abstract terms\n",
    "are known as hypernyms and more specific terms are hyponyms. This tree can be traced all\n",
    "the way up to a root hypernym\n",
    "\n",
    "Hypernyms provide a way to categorize and group words based on their similarity to each\n",
    "other. The Calculating WordNet Synset similarity recipe details the functions used to calculate\n",
    "the similarity based on the distance between two words in the hypernym tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('set.n.01')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bracket.n.01'),\n",
       " Synset('chess_set.n.01'),\n",
       " Synset('choir.n.02'),\n",
       " Synset('conjugation.n.03'),\n",
       " Synset('core.n.01'),\n",
       " Synset('dentition.n.02'),\n",
       " Synset('field.n.12'),\n",
       " Synset('field.n.13'),\n",
       " Synset('field.n.15'),\n",
       " Synset('intersection.n.04'),\n",
       " Synset('manicure_set.n.01'),\n",
       " Synset('octet.n.03'),\n",
       " Synset('pair.n.01'),\n",
       " Synset('portfolio.n.02'),\n",
       " Synset('quartet.n.03'),\n",
       " Synset('quintet.n.04'),\n",
       " Synset('score.n.04'),\n",
       " Synset('septet.n.03'),\n",
       " Synset('sextet.n.04'),\n",
       " Synset('singleton.n.02'),\n",
       " Synset('suite.n.04'),\n",
       " Synset('synset.n.01'),\n",
       " Synset('threescore.n.01'),\n",
       " Synset('trio.n.04'),\n",
       " Synset('union.n.08')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('abstraction.n.06'),\n",
       "  Synset('group.n.01'),\n",
       "  Synset('collection.n.01'),\n",
       "  Synset('set.n.01'),\n",
       "  Synset('core.n.01')]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypernym_paths() method returns a list of lists, where each list starts at the root\n",
    "hypernym and ends with the original Synset. Most of the time, you'll only get one nested\n",
    "list of Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
