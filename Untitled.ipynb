{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from numpy import array\n",
    "from unicodedata import normalize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data in a way such that it preserve unicode german characters\n",
    "def load_doc(filename):\n",
    "    file = open(filename,mode=\"rt\",encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "#Splitting the document into the sentence pair\n",
    "def to_pair(doc):\n",
    "    lines = doc.strip().split(\"\\n\")\n",
    "    pairs = [line.split(\"\\t\") for line in lines]\n",
    "    return pairs\n",
    "\n",
    "#Removing text other than pair of german and english\n",
    "def removing_extra(group):\n",
    "    pair_group = []\n",
    "    for pair in group:\n",
    "        pair = pair[0:2]\n",
    "        pair_group.append(pair)\n",
    "    return pair_group\n",
    "\n",
    "#Cleaning the text(list of text)\n",
    "def clean_text(text_list):\n",
    "    #Creating a empty list in which all the cleaned text will be appended\n",
    "    clean = list()\n",
    "    #For removing the punctuation from the text\n",
    "    re_punc = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "    #For removing all the non printable characters\n",
    "    re_print = re.compile(\"[^%s]\" % re.escape(string.printable))\n",
    "    for pair in text_list:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            #Normalizing the unicode characters\n",
    "            line = normalize(\"NFD\", line).encode(\"ascii\",\"ignore\")\n",
    "            line = line.decode(\"UTF-8\")\n",
    "            #Tokenizing on white space in text\n",
    "            line = line.split()\n",
    "            #Convert the text to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            #Removing the punctuation from each token\n",
    "            line = [re_punc.sub(\"\", w) for w in line]\n",
    "            #Removing the non-printable chars form each token\n",
    "            line = [re_print.sub(\"\", w) for w in line]\n",
    "            #Removing the tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            #After cleaning the pair appending the pair to clean_pair list\n",
    "            clean_pair.append(\" \".join(line))\n",
    "        #Now appending the cleaned pair to the clean list\n",
    "        clean.append(clean_pair)\n",
    "    return array(clean)\n",
    "\n",
    "#Now saving the list of clean sentences to file in our storage\n",
    "def save_clean(data_list,filename):\n",
    "    #Dumping sentences in a file(file opened in writebyte mode)\n",
    "    dump(data_list,open(filename,\"wb\"))\n",
    "    print(\"Saved: %s\"%filename)\n",
    "    \n",
    "#Checking the length of cleaned data(number of pairs)\n",
    "def checking_length(data):\n",
    "    length = len(data)\n",
    "    return length\n",
    "\n",
    "#As the number of cleaned pairs we have is very lagre around 208486 so insted of fitting the model to the dataset which will--\n",
    "#--be a quite expensive task(huge computation) we will insted train our model on a subset of our data taking fewer pairs as compared to the number of pairs we have\n",
    "#A function for loading the data and it will be helpful in loading the files which we save for purpose of later use in save_clean function\n",
    "def loading_data(filename):\n",
    "    return load(open(filename,\"rb\"))\n",
    "\n",
    "#Now as our data cleaning,splitting the data into training and testing parts is over what we can do is start building our translation network\n",
    "#Creating a tokenizer\n",
    "def create_tokenizer(text_data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    return tokenizer\n",
    "\n",
    "#For finding the maximum sequence length\n",
    "def max_length(data):\n",
    "    return max(len(dat.split()) for dat in data)\n",
    "\n",
    "#We will be now preparing the training dataset.Each input and output sequence must beencoded to integers and padded to the maximum phrase length\n",
    "#--this is because we will use a word embedding for the input sequences and one hot encode the output sequences\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer,length,data):\n",
    "    #Converting the array of text to sequence of integer on basis of their word-index position\n",
    "    X = tokenizer.texts_to_sequences(data)\n",
    "    #Length returned by max_length function\n",
    "    #Now padding the sequence created(padding will be post zeros filled at end for shorter sequence)\n",
    "    X = pad_sequences(X, maxlen=length, padding=\"post\")\n",
    "    return X\n",
    "\n",
    "#The output sequence will be one hot encoded as the model will predict the probability of each word in the vocabulary as output\n",
    "def encode_output(sequences,vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence,num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "        y = array(ylist)\n",
    "        y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "#Creating the model(encoder decoder)\n",
    "def define_model(source_vocab,target_vocab,source_len_pad,target_len,n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(source_vocab,n_units,input_length=source_len_pad,mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(target_len))\n",
    "    model.add(LSTM(n_units,return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(target_vocab, activation=\"softmax\")))\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "    model.summary()\n",
    "    plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "208486\n",
      "[['hi' 'hallo']\n",
      " ['hi' 'gru gott']\n",
      " ['run' 'lauf']\n",
      " ...\n",
      " ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker'\n",
      "  'wenn jemand der deine herkunft nicht kennt sagt dass du wie ein muttersprachler sprichst bedeutet das dass man wahrscheinlich etwas an deiner sprechweise bemerkt hat das erkennen lie dass du kein muttersprachler bist mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an']\n",
      " ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker'\n",
      "  'wenn jemand fremdes dir sagt dass du dich wie ein muttersprachler anhorst bedeutet das wahrscheinlich er hat etwas an deinem sprechen bemerkt dass dich als nichtmuttersprachler verraten hat mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an']\n",
      " ['doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people and out of the few hundred that there are but a dozen or less whom he knows intimately and out of the dozen one or two friends at most it will easily be seen when we remember the number of millions who inhabit this world that probably since the earth was created the right man has never yet met the right woman'\n",
      "  'ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht dass ein mensch nur gelegenheit hat mit ein paar hundert anderen bekannt zu sein von denen ihm nur ein dutzend oder weniger nahesteht darunter hochstens ein oder zwei freunde dann erahnt man eingedenk der millionen einwohner dieser weltleicht dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist']]\n"
     ]
    }
   ],
   "source": [
    "doc = load_doc(\"deu.txt\")\n",
    "pairs = to_pair(doc)\n",
    "pair = removing_extra(pairs)\n",
    "clean = clean_text(pair)\n",
    "save_clean(clean,\"english-german.pkl\")\n",
    "print(checking_length(clean))\n",
    "#Generally we have a total of 208486 pairs to english,german pairs and towards the end the length of pairs are quite bigger\n",
    "print(clean[1:208486])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german(combined).pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset which we actually created\n",
    "data = loading_data(\"english-german.pkl\")\n",
    "#Reducing the data size\n",
    "num_sentences = 15000\n",
    "data = data[:num_sentences, :]\n",
    "#Randomly shuffling the data\n",
    "shuffle(data)\n",
    "#Splitting the data into train and test data\n",
    "train,test = data[:12000],data[12000:]\n",
    "#Saving the training and testing data\n",
    "save_clean(data,\"english-german(combined).pkl\")\n",
    "save_clean(train,\"english-german-train.pkl\")\n",
    "save_clean(test,\"english-german-test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOading the train and test data\n",
    "data = loading_data(\"english-german(combined).pkl\")\n",
    "train = loading_data(\"english-german-train.pkl\")\n",
    "test = loading_data(\"english-german-test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['whats tom done', 'was hat tom getan'],\n",
       "       ['dont misbehave', 'benehmt euch nicht daneben'],\n",
       "       ['he is my type', 'er ist mein typ'],\n",
       "       ...,\n",
       "       ['hes intelligent', 'er ist klug'],\n",
       "       ['you can try', 'du kannst es versuchen'],\n",
       "       ['youre arrogant', 'sie sind uberheblich']], dtype='<U527')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2960\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 4648\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "#Basically we are using the full data for creating the vocabulary of boyh english and german(alternately we can also use only the train data)\n",
    "#Preparing the english tokenizer\n",
    "eng_tokenizer = create_tokenizer(data[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(data[:, 0])\n",
    "print(\"English Vocabulary Size: %d\"%eng_vocab_size)\n",
    "print(\"English Max Length: %d\"%(eng_length))\n",
    "#Preparing the german tokenizer\n",
    "ger_tokenizer = create_tokenizer(data[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(data[:, 1])\n",
    "print(\"German Vocabulary Size: %d\"%ger_vocab_size)\n",
    "print(\"German Max Length: %d\"%(ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the training and testing sequences\n",
    "#First we encode both the input and output sequence of word_index value and then encode output by one hot\n",
    "trainX = encode_sequences(ger_tokenizer,ger_length,train[:,1])\n",
    "trainY = encode_sequences(eng_tokenizer,eng_length,train[:,0])\n",
    "trainY = encode_output(trainY,eng_vocab_size)\n",
    "testX = encode_sequences(ger_tokenizer,ger_length,test[:,1])\n",
    "testY = encode_sequences(eng_tokenizer,eng_length,test[:,0])\n",
    "testY = encode_output(testY,eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "checkpoint = ModelCheckpoint(\"model.h5\",monitor=\"val_loss\",verbose=1,save_best_only=True, mode=\"min\")\n",
    "model.fit(trainX,trainY,epochs=30, batch_size=64,validation_data=(testX, testY),callbacks=[checkpoint],verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
